{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99f461e9-1692-44ca-817b-3756d2ec4917",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Importing all libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f6ab71a-36f1-497d-891f-a642af9b3bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import srsly\n",
    "import typer\n",
    "import random\n",
    "import warnings\n",
    "import string\n",
    "import PyPDF2\n",
    "import csv\n",
    "import glob\n",
    "import docx2txt\n",
    "import pandas as pd\n",
    "#import PharseMatcher\n",
    "#import configparserparser\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.training import Example\n",
    "from spacy.lang.en import English\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy.training import docs_to_json\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  \n",
    "\n",
    "from datetime import datetime\n",
    "import datefinder\n",
    "from dateutil.parser import parse\n",
    "\n",
    "from common import create_patterns\n",
    "from common import create_patterns_org\n",
    "from common import create_patterns_name\n",
    "from common import create_patterns_email\n",
    "from common import create_patterns_mobnum\n",
    "from common import create_patterns_DOB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e56216-7c8f-4d4f-83a0-44aed5ceda0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ALL GetAll_docx and pdf from home folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70eb3412-d929-474c-8731-7c473c102e54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "#import Pypdf\n",
    "import glob\n",
    "import re\n",
    "import itertools # Used to merge list of lists\n",
    "\n",
    "\n",
    "## Author: Deepinder\n",
    "## Description: getallfilenames returns all files as a list in the specified path, also returns all files from subfolders \n",
    "##              if the getall parameter is passed as \"Y\"\n",
    "## How to Call: print(getallfilenames(path=\"/home\", getall='Y'))\n",
    "## Comments: 1) FIXED: The method was looping multiple times in the subfolders is fixed, So i am not expecting any duplicates\n",
    "##           2) FIXED: Changed the code to have local variable fn instead of having it declared globally.\n",
    "\n",
    "\n",
    "def getallfilenames(path = 'home', getall = 'N'): # Definition of the Method\n",
    "    fn = []\n",
    "    if (path=='home'): # Get CURRENT DEFAULT PATH\n",
    "        path = os.getcwd()    # Get current Directory\n",
    "    elif (os.path.exists(path)==False): #Check if the passed path exists or exit with return.\n",
    "        print ('Path Not found')\n",
    "        return\n",
    "    fn.append(glob.glob(path+'/*.doc*'))\n",
    "    fn.append(glob.glob(path+'/*.pdf'))\n",
    "    if getall == 'Y':\n",
    "        for roots,dirs,files in os.walk(path): # This code is to find the directories\n",
    "            if (roots!=path) and (path+\"/.\" not in roots) and (len(files)>0): # Excluding any folders which starts with .(dot)\n",
    "#                print(\"SS\"+path,roots,len(dirs),len(files)) # This code is to find the directories\n",
    "                fn.append(glob.glob(roots+'/*.doc*'))\n",
    "                fn.append(glob.glob(roots+'/*.pdf'))\n",
    "    \n",
    "# Below code is to merge all list of lists into one list and remove blank list\n",
    "    filenames = list(filter(None, fn)) ## Remove blank list \n",
    "    filenames = list(itertools.chain.from_iterable(filenames)) ## Remove list of list\n",
    "#    filenames = list(set(filenames)) ## remove Duplicates ## Removed this because i am not expecting any duplicates\n",
    "    return (filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc01be7-f330-4aeb-a136-85775acd409d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Method convertfileintotext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c87737af-6362-45bf-8afe-6f618ff4dc52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Author: Sahil\n",
    "#Description: convertfile into text returns text in the docx or pdf file, It extracts the file name from the path.\n",
    "#             Paramter pass is the filepath \n",
    "# How to call:(convertfileintotxt(filepath))\n",
    "# Comments: 1) It moves to a function to extract docs and return txt, same thing goes if we fohdn a pdf file \n",
    "def convertfileintotxt(filepath):\n",
    "    filename = os.path.basename(filepath) # it will pick up the filename from the end of the path.\n",
    "    print(filename)\n",
    "    if filename.endswith(\".docx\"): #if filename ends with docx \n",
    "        ret_text=Extract_docx_file(filepath) #it will give call to extract docxfile function \n",
    "        return ret_text\n",
    "    elif filename.endswith(\".pdf\"): #if filename ends with pdf \n",
    "        Extract_pdf_file(filename)#it will give call to extract pdf_file function\n",
    "        ret_text=Extract_pdf_file(filepath)\n",
    "        return ret_text\n",
    "    else:\n",
    "         print(\"Not a proper format file\")  #or else it will print not a proper format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c642ed01-b236-4b6d-8fb3-33d3e417565b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Methid extract docxfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb1820e6-3d2c-4e8e-a619-8779226659ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  function to open a docx file \n",
    "## Author: Sahil\n",
    "## Description:Extract docx file takes the docx file and uses docx2txt to read the file in return file in a variable called resume \n",
    "## How to Call:this function is called in convert file to text from there it will be called \n",
    "##Comments : Used python dox2txt to read doxx file in python \n",
    "def  Extract_docx_file(file):\n",
    "        resume = docx2txt.process(file) #docx2txt used to read docx file \n",
    "        return resume # return resume when docx file found \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c5369a-ac92-49d6-82f9-4cd3a42e0d8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Method Extract_pdf_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c90b202c-8a54-4a60-929d-e39752b8bd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  function to open a pdf file \n",
    "## Description:Extract pdf file method takes the pdf file and uses pdf.reader to read the file in return file in a variable called resume \n",
    "## How to Call:this function is called in convert file to text from there it will be called \n",
    "##Comments : Used python pdffilereader to read pdf file in python \n",
    "\n",
    "def Extract_pdf_file(file):\n",
    "    resume=\"\"\n",
    "    output = []\n",
    "    \n",
    "    if (os.path.isfile(file)==True):\n",
    "        pdf_file = open(file,\"rb\")\n",
    "        pdfReader = PyPDF2.PdfFileReader(pdf_file)\n",
    "        count= pdfReader.numPages\n",
    "        for i in range(count):\n",
    "            page = pdfReader.getPage(i)\n",
    "            output.append(page.extractText())\n",
    "        for element in output:\n",
    "            resume = resume + str(element )\n",
    "        print(\"\\nSuccessfully Read\")\n",
    "        return resume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c9446f-8b18-499c-a47b-2e21f50f1422",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Method Removecharacpunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91d977f5-0eae-4bf8-ad57-7ef573e04339",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#function to remove characters such as , : - + spaces  \n",
    "## Author: Sahil\n",
    "## Description:Removecharpunc removes all the punctions from the resume and returns txtdata in remchrpunc\n",
    "## How to Call: removecharacpunc(textdata)\n",
    "## Comments: The method is remove all pipes , and all other things from the textdata \n",
    "\n",
    "def removecharacpunc(textdata):\n",
    "    punc = '''!()[]{}();:'\",<>?#$%^&*_~'''\n",
    "    res = re.sub(' +', ' ', textdata)\n",
    "    res= re.sub(', ', ' ', res)\n",
    "    res=re.sub(\"â€™\",\"\",res)\n",
    "#     res = res.replace(\"'\", \"\")\n",
    "#     res = res.replace(\"(\", \"\")\n",
    "#     res = res.replace(\")\", \"\")\n",
    "    res = res.replace('|', \" \")\n",
    "    for ele in res:\n",
    "        if ele in punc:\n",
    "             remchrpunc = res.replace(ele, \"\")\n",
    "    return remchrpunc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efae0ec3-e176-4b41-94f9-1813f857c727",
   "metadata": {},
   "source": [
    "### Method removetabsnemptylines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f11a32c-8b46-4b22-8098-7e15aadc6583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove tab and extra new line\n",
    "def removetabsnemptylines(remchar):\n",
    "    \n",
    "    lines = remchar.split(\"\\n\")\n",
    "    non_empty_lines = [line for line in lines if line.strip() != \"\"]\n",
    "\n",
    "    string_without_empty_lines = \"\"\n",
    "    for line in non_empty_lines:\n",
    "          string_without_empty_lines += line + \"\\n\"\n",
    "    res=string_without_empty_lines.replace('\\t', '')\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c494ee2-5029-4a7a-8531-d935e2749f73",
   "metadata": {},
   "source": [
    "### 7 Method Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "432ee3a3-375e-4b70-b328-19a9390e7544",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Author:Sahil\n",
    "#Description: removing all the stop words from the doc objec....We would be using spacy attriubte nto remove stopwords i.e token.is_stop\n",
    "#How to call a function: removestopwords(doc)---where doc is the input string \n",
    "#Comments: Hear I have removed all the stop words and also got the lenth of my doc object and my cleaned object.\n",
    "\n",
    "\n",
    "\n",
    "def removestopwords(doc):\n",
    "    stopwrd_str = \"\"  # making a string \n",
    "    my_doc_cleaned = [token for token in doc if not token.is_stop and not token.is_punct]\n",
    "    for token in my_doc_cleaned:\n",
    "        stopwrd_str=stopwrd_str + ' ' + token.text    \n",
    "    return stopwrd_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e202613-54b6-48e0-b99f-54232b9ab825",
   "metadata": {},
   "source": [
    "### 8.Method replace lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "38576d8e-ff80-4169-b3cb-9563f0fef6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Sahil \n",
    "# Description:Replace all the words with the lemma gives the output into a list\n",
    "##How to call: replacelemma(doc,res)---> where doc is a doc object and res is a string.\n",
    "##Comments: 1. I have made the fuction by using a temporary doc object while in real we would be passing resume doc object.\n",
    "##          2. I have used the replace command to replace one string to onother and then store it in temp  \n",
    "## Coomment: Hear lemmatization is done as playing becomes play From becomes from.\n",
    "\n",
    "\n",
    "\n",
    "#res = (\"I am playing football and will be going to school From Playing tommorow\")\n",
    "#print(type(res))\n",
    "#doc = nlp(res)\n",
    "#print(type(doc))\n",
    "def replacelemma(doc,res):\n",
    " \n",
    "    for token in doc:\n",
    "        ntoken = str(token)\n",
    "        nlemma = str(token.lemma_)\n",
    "        if ntoken != nlemma:\n",
    "            #print(ntoken + ' ' + nlemma)\n",
    "            temp=res.replace(ntoken, nlemma)\n",
    "            res=temp\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0355c3-9773-45d0-b840-436363d465c7",
   "metadata": {},
   "source": [
    "## 9. Method read company using csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c439d66e-d0fd-4e03-b5d5-1c8590a2f928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Sahil \n",
    "# Description:Get all the company name from the csv df and put it into a list and running a for loop to match it with input string and\n",
    "##            gives the output into a list\n",
    "##How to call: readskills(doc)---> where doc is input string \n",
    "##Comments: 1. I have made the fuction by using a temporary doc object while in real we would be passing resume doc object.\n",
    "##          2. I have to add more cpmapnay names to the Csv to it should not miss any of it\n",
    "## Coomment: Other method of extracting company names are pattern way  \n",
    "\n",
    "#Right\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "#doc2=nlp(\"Aaditya techonologies and Impluse software solutions are two big companies with the help of wal mart company \")\n",
    "def readcompanyname(doc):\n",
    "   \n",
    "    matcher = PhraseMatcher(nlp.vocab)\n",
    "    tokens = [token.text for token in doc]\n",
    "    df = pd.read_csv(\"Maincsvv.csv\")\n",
    "\n",
    "    lst= [] \n",
    "    for i in df[\"Company_Name\"]:\n",
    "        lst.append(i)     \n",
    "    #print(lst)\n",
    "\n",
    "    #obtain doc object for each word in the list and store it in a list\n",
    "    \n",
    "    patterns = [nlp(cn) for cn in lst]\n",
    "    #add the pattern to the matcher\n",
    "    matcher.add(\"compnay_name\", patterns)\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    cnlist = []\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        cnlist.append(span.text)\n",
    "    return cnlist\n",
    "    complist = list(dict.fromkeys(cnlist))\n",
    "   # print(aalist)\n",
    "    return complist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e36d18c-f26d-4582-b4ff-9e15dd90f432",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 10.Method readskills "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fdde2e1b-91a9-4e91-9fbc-3c62b4615df4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### # Author: Sahil \n",
    "# Description:Get all the skills from the csv df and put it into a list and running a for loop to match it with input string and\n",
    "##            gives the output into a list\n",
    "##How to call: readskills(doc)---> where doc is input string \n",
    "##Comments: 1. I have made the fuction by using a temporary doc object while in real we would be passing resume doc object.\n",
    "##          2. I have to add more skills to the Csv to it should not miss any of it\n",
    "## COmment 1. Other method to ffind skills is make patterns of all the skills in the common.py and then find it with matcher]\n",
    "\n",
    "\n",
    "#doc = nlp(\"This is the biggest Java programming core php seminar, Java is good in  Sql Server I have come Core Java hear HTML5 to learn CSS MySQL\")\n",
    "def readskills(doc):\n",
    "    matcher = PhraseMatcher(nlp.vocab) #PhraseMatcher lets you efficiently match large terminology lists.\n",
    "    tokens = [token.text for token in doc] # Iterating through the loop to store token.text in a list\n",
    "   \n",
    "    df = pd.read_csv(\"Maincsvv.csv\")      # reading data from a csv file and storing it in dataframe             \n",
    "    lst= []                               # making a list to store the skills\n",
    "    for i in df[\"Skills\"]: # for data from  dataframe column 'Skills'\n",
    "        lst.append(i)# appending all column values to a list \n",
    "    patterns = [nlp(cn) for cn in lst] \n",
    "    matcher.add(\"Skills\",patterns) #adding all the skills to the pattern\n",
    "    matches = matcher(doc)\n",
    "    \n",
    "    cnlist = []\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        cnlist.append(span.text)\n",
    "    skilllist = list(dict.fromkeys(cnlist))\n",
    "    return skilllist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8b0911-af7b-4e79-861e-067a79e86769",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### ## 11. Method to Extract email.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3d99a6d2-009f-48fc-acb5-0bec611aabb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "##Author: Sahil \n",
    "##Description:Get all the email address from the inputstring i.e doc object.\n",
    "##How to call: extractemail(doc)--->doc is the input paramter.\n",
    "#Comments: 1. It will extract all the emails address from the inputstring and store it in a list and return the list  \n",
    "##         2. Right now we are not taking the whole resume doc object but passing a temporary doc object which has emails in it \n",
    "##            to check if it is working.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#doc = nlp(\"sahil@yahoo.com yhyhy as.Talend@gmail.com 813-204-0866 | sahil_hodekar.Sahil@gmail.com IND123@gmail.com service@yahoo.in sahil@offsoar.com\")#Making a nlp doc object\n",
    "# # function to extract email\n",
    "def extractemail(doc):      # takes doc object as input parameter.\n",
    "    email_lst = []           # making a list to store the email address\n",
    "    for token in doc:       # Iterate the loop for all the tokens in doc \n",
    "        if token.like_email: # Matching the tokens which are email through `like_email` attribute\n",
    "            email_lst.append(token.text)  # append the matched tokens to a list\n",
    "    return email_lst         # returns list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82336353-5f34-49a0-bb39-925867233a5e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### #12.Method mobile Number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cd1cbb15-f12b-4973-ad2d-ddde7bb7f7d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Author: Sahil \n",
    "# Description: Get the mobile number from the input string i.e doc object.\n",
    "##How to call: extractnumbers(doc)--->doc is the input paramter.\n",
    "#Comments: 1. It will extract all the numbers from the inputstring and store it in a list and return the list  \n",
    "##         2. Right now we are not taking the whole resume doc object but passing a temporary doc object which has mobile numbers in it \n",
    "##            to check if it is working\n",
    "##         3. Problems faced using like.num spacy atttribute is IT does not extract all the numbers like for eg. Numbers in 348-345-433 and (345)434\n",
    "##         4.It picks numbers like +919967011417 9987177046\n",
    "## Solution  -- I have to make patterns using regex to extract mobile numbers. \n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")#Loading a default spacy model \n",
    "#doc = nlp(\"(I am having (456)234567 mobile 348-234-345 number 99387177046 But earlier +3439439393 my mobile 41234344333 number was +919967011417\")#Making a nlp doc object \n",
    "def extractmobnumbers(doc): #putting the input as doc object.\n",
    "    num_lst=[] #making a list to store the number \n",
    "    for token in doc: #For all the tokens in doc it will take a for loop \n",
    "        if token.like_num: #using spacy default .like_num it finds the number \n",
    "            num = (token)\n",
    "            if(len(num) >=10):#Checking if the length of the token is more than 10 digits \n",
    "                num_lst.append(num)#if the number is more than 10 digit or 10 digit add it to the numlist\n",
    "    \n",
    "    phno=phoneno(doc)\n",
    "    num_lst.append(phno)\n",
    "    return num_lst#returns numlist at then end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d709d175-7600-4be4-b26e-9a4ba4706934",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Phone: +14157876954']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = Extract_docx_file('7499495.docx')\n",
    "#print(res)\n",
    "extractname(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2cf46d3a-345a-421c-a120-5b849067680f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Phone: +14157876954']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractname(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "77680c2f-4e19-484f-853a-cbec32d6e89b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (<ipython-input-42-376ed7f6a5f3>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-42-376ed7f6a5f3>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    res = convertfileintotxt('C:\\Users\\sahil\\Nova31082021\\7499495.docx')\u001b[0m\n\u001b[1;37m                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "res = convertfileintotxt('C:\\Users\\sahil\\Nova31082021\\7499495.docx')\n",
    "print(res)\n",
    "extractname(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87f2e24-59f6-4f6e-ba84-f4832541b3c2",
   "metadata": {},
   "source": [
    "### 13 Extract first name last name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a91eb4-1505-4428-a7b3-d43c1c7f79f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "707b8423-1129-4806-95cf-eb1a15405562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Author: Sahil\n",
    "## Description: This function helps us extracting candidate name from the resume \n",
    "## How to Call: (extractname(res) ---res string to be passed as input paramter.\n",
    "## Comments: 1) Functions extract first line of the String and if first line is the \"RESUME\" or CURRICULUM VITAE \n",
    "##                it extract second line and extract candidate name from the second line.\n",
    "\n",
    "def extractname(res): \n",
    "    listname=[]\n",
    "    candidatenname = res.split('\\n', 1)[0] # this code is use to extract the first line from the string\n",
    "    if candidatenname == \"RESUME\" or candidatenname == \"CURRICULUM-VITAE\" or candidatenname ==\"CURRICULUM VITAE\":# checking if the first line is resume or cirriculm vitae\n",
    "        candidatenname = res.split('\\n',2)[1] # # this code is use to extract the second line from the string\n",
    "        candidatenname = candidatenname.strip()# it removes starting and end spaces \n",
    "        if candidatenname ==\"Name\":\n",
    "            candidatenname = res.split('\\n',3)[0] #this code is use to extract the third line from the string\n",
    "    listname.append(candidatenname)\n",
    "    return listname\n",
    "extractname(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df40124e-5a7a-4b25-a1d7-eff83c6a3aee",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 24. Read Qualifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09b9c50f-c6d6-4d1a-b2a4-1d2056f78125",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Author: Sahil \n",
    "# Description:Get all the qualification into a list and make  a for loop to match it with input string and\n",
    "##            gives the output into a list\n",
    "##How to call: qualtifications(doc)---> where doc is input string \n",
    "##Comments: 1. I have made the fuction by using a temporary doc object while in real we would be passing resume doc object.\n",
    "##          2. I have to add more degree names to the list should not miss any of it\n",
    "  \n",
    "\n",
    "#doc = nlp(\"I enjoy B.S.C working in Purplejack M.C.A more than I used to be in BSC (COMP SCI.) . To work in Apple is my Great \")\n",
    "def extract_qualification(doc):\n",
    "    \n",
    "    matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "    Qualification = ['B.S.C' , 'B.A','M.C.A','Bachelor of Engineering','Higher School Certificate','Secondary School Certificate' ,\n",
    "             'BSC (COMP SCI.)','BCA','B. Tech','Bachelor in technology','Bachelor of Science (BS)',\n",
    "            'M.Sc. in Computers','M.Sc.','M. Tech','MCA','Masters in Computer Applications','Master in Systems Management',\n",
    "            'M.S',' Bachelor in Computer Applications','B.C.A','Bachelor in Computer Science',\n",
    "            'Masters in Computer Science']\n",
    "\n",
    "#obtain doc object for each word in the list and store it in a list\n",
    "    patterns = [nlp(cn) for cn in Qualification] #making a doc object of all the words.\n",
    "#add the pattern to the matcher\n",
    "    matcher.add(\"Qualification_pattern\", patterns) \n",
    "    matches = matcher(doc)\n",
    "    qualification_list = []\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        qualification_list.append(span.text)\n",
    "    return qualification_list "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce1556c-4f1d-4d59-9587-7266e9d9e987",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DATE PATTERN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0591a53-fbaf-4c01-b389-231a32692e97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Author: Sahil\n",
    "## Description: Extract dates method has used patterns to take out all dates from the passed doc file \n",
    "## How to Call: extractdates(doc)\n",
    "## Comments: 1) FIXED: The method was picking out anything that comes before digit like sahil26 or c-11, so changed patterns \n",
    "##           2) FIXED: as after making patterns the patterns wer repating so used dict.fromkeys to remove repated words \n",
    "##           3) FIXED: Then  IT MATCHES IF found date is Jan 97 then it will change to Jan1997 or if the date is Jan18 it will print Jan2018\n",
    "def extractdates(doc):\n",
    "    matcher = Matcher(nlp.vocab)  \n",
    "    date_pattern1 = [{'TEXT':{'REGEX':r'^Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May?|June?|July?|Aug(?:ust)?|Sept?(?:ember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?|Dec(?:ember)$'}},{'TEXT':{'REGEX':r'^\\d{2,4}$'}}]\n",
    "    matcher.add('DATE_PATTERN_1', [date_pattern1])\n",
    "        \n",
    "    date_pattern2 = [{'TEXT':{'REGEX':r'^\\d{2}$'}},{'TEXT':{'REGEX':r'^Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May?|June?|July?|Aug(?:ust)?|Sept?(?:ember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?'}},{'TEXT':{'REGEX':r'^\\d{2,4}$'}}]\n",
    "    matcher.add('DATE_PATTERN_2', [date_pattern2])\n",
    "\n",
    "    date_pattern3 = [{'TEXT':{'REGEX':r'^\\d{1}$'}},{'TEXT':{'REGEX':r'^Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May?|June?|July?|Aug(?:ust)?|Sept?(?:ember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?'}},{'TEXT':{'REGEX':r'^\\d{2,4}$'}}]\n",
    "    matcher.add('DATE_PATTERN_3', [date_pattern3])\n",
    "\n",
    "    date_pattern4 = [{'TEXT':{'REGEX':r'^Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May?|June?|July?|Aug(?:ust)?|Sep?(?:tember)?|Sept?(?:ember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?'}},{'TEXT':{'REGEX':r'^\\d{2}$'}},{'TEXT':{'REGEX':r'^\\d{2,4}$'}}]\n",
    "    matcher.add('DATE_PATTERN_4', [date_pattern4])\n",
    "\n",
    "    date_pattern5 = [{'TEXT':{'REGEX':r'^Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May?|June?|July?|Aug(?:ust)?|Sept?(?:ember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?'}},{'TEXT':{'REGEX':r'^\\d{2}(?:st|nd|rd|th)?$'}},{'TEXT':{'REGEX':r'^\\d{2,4}$'}}]\n",
    "    matcher.add('DATE_PATTERN_5', [date_pattern5])\n",
    "    \n",
    "    date_pattern6 = [{'TEXT':{'REGEX':r'^Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May?|June?|July?|Aug(?:ust)?|Sept?(?:ember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?'}},{'TEXT':{'REGEX':r'^\\d{1}(?:st|nd|rd|th)?$'}},{'TEXT':{'REGEX':r'^\\d{2,4}$'}}]\n",
    "    matcher.add('DATE_PATTERN_6', [date_pattern6])\n",
    " \n",
    "\n",
    "    date_pattern8 = [{'TEXT':{'REGEX':r'^(0[1-9]|[12][0-9]|3[01])[- /.](0[1-9]|1[012])[- /.](19|20)\\d\\d$'}}]\n",
    "    matcher.add('DATE_PATTERN_8', [date_pattern8])  \n",
    "    \n",
    "    date_pattern11 = [{'TEXT':{'REGEX':r'^\\d{1,2}?[/:\\']\\d{2,4}$'}}] # eg 02/2021 8/2021\n",
    "    matcher.add('DATE_PATTERN_8', [date_pattern11])\n",
    "\n",
    "    date_pattern9 = [{'TEXT':{'REGEX':r'([0]?\\d|[1][0-2])[/-]([0-3]?\\d)[/-]([1-2]\\d{3}|\\d{2})'}}]\n",
    "    matcher.add('DATE_PATTERN_9', [date_pattern9])\n",
    "    \n",
    "    \n",
    "    date_pattern11 = [{'TEXT':{'REGEX':r'^\\d{2}(?:st|nd|rd|th)?$'}},{'TEXT':{'REGEX':r'^Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May?|June?|July?|Aug(?:ust)?|Sept?(?:ember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?$'}},{'TEXT':{'REGEX':r'^\\d{2,4}$'}}]\n",
    "    matcher.add('DATE_PATTERN_11', [date_pattern11])\n",
    "    \n",
    "#     date_pattern12 = [{'TEXT':{'REGEX':r'^Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May?|June?|July?|Aug(?:ust)?|Sept?(?:ember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?|Dec(?:ember)'}}]\n",
    "#     matcher.add('DATE_PATTERN_12', [date_pattern12])\n",
    "    \n",
    "\n",
    "#   date_pattern10 = [{'IS_DIGIT': True},\n",
    "#            {'IS_PUNCT': True},\n",
    "#            {'IS_DIGIT': True},\n",
    "#            {'IS_PUNCT': True},\n",
    "#            {'IS_DIGIT': True}]\n",
    "\n",
    "#     matcher.add('DATE_PATTERN_10', [date_pattern10])  \n",
    "    \n",
    "    \n",
    "    lstdate=[] \n",
    "    matches = matcher(doc)\n",
    "    \n",
    "    def string_set(datelist):\n",
    "         return set(i for i in datelist \n",
    "                if not any(i in s for s in datelist if i != s))\n",
    "\n",
    "    for match_id, start, end in matches:\n",
    "        string_id = nlp.vocab.strings[match_id]\n",
    "        span = doc[start:end]\n",
    "        lstdate.append(span.text)\n",
    "    datelist = list(dict.fromkeys(lstdate))\n",
    "    datelist1=string_set(datelist)\n",
    "    \n",
    "    def string_set(datelist):\n",
    "         return set(i for i in datelist \n",
    "                if not any(i in s for s in datelist if i != s))\n",
    "\n",
    "    str1=\"\" \n",
    "    lsts=[]\n",
    "    for ele in datelist1:\n",
    "        if (re.match(\"(Jan)([0-9]{2})\",ele) or re.match(\"^(January[\\d][\\d])\",ele)\n",
    "            or re.match(\"(Feb)([0-9]{2})\",ele) or re.match(\"(^February[\\d\\d])\",ele)\n",
    "            or re.match(\"(Mar)([0-9]{2})\",ele) or re.match(\"(^March[\\d\\d])\",ele)\n",
    "            or re.match(\"(Apr)([0-9]{2})\",ele) or re.match(\"(^April[\\d\\d])\",ele)  \n",
    "            or re.match(\"(May)([0-9]{2})\",ele) or re.match(\"(^June[\\d\\d])\",ele)\n",
    "            or re.match(\"(July)([0-9]{2})\",ele) or re.match(\"(^Aug[\\d\\d])\",ele)\n",
    "            or re.match(\"(August)([0-9]{2})\",ele) or re.match(\"(^Sep[\\d\\d])\",ele)\n",
    "            or re.match(\"(Sept)([0-9]{2})\",ele) or re.match(\"(^September[\\d\\d])\",ele)\n",
    "            or re.match(\"(Oct)([0-9]{2})\",ele) or re.match(\"(^October[\\d\\d])\",ele)\n",
    "            or re.match(\"(Nov)([0-9]{2})\",ele) or re.match(\"(^November[\\d\\d])\",ele)\n",
    "            or re.match(\"(Dec)([0-9]{2})\",ele) or re.match(\"(^December[\\d\\d])\",ele)\n",
    "           ): \n",
    "            lsts.append(ele)\n",
    "    \n",
    "    for ele in lsts:\n",
    "        datetwodigit=ele[-2:]\n",
    "        if int(datetwodigit) > 50: \n",
    "            ele = ele[:-2] + \"19\" + str(datetwodigit)\n",
    "        elif int(datetwodigit)<50:\n",
    "            ele = ele[:-2] + \"20\" + str(datetwodigit)\n",
    "    \n",
    "        \n",
    "    lstappend=[]\n",
    "    for ele in lsts:\n",
    "        datetwodigit=ele[-2:]\n",
    "        if int(datetwodigit) > 50: \n",
    "            ele = ele[:-2] + \"19\" + str(datetwodigit)\n",
    "            lstappend.append(ele)\n",
    "        elif int(datetwodigit)<50:\n",
    "            ele = ele[:-2] + \"20\" + str(datetwodigit)\n",
    "            lstappend.append(ele)\n",
    "        \n",
    "    remlst = list(set(datelist)^set(lsts))\n",
    "     \n",
    "\n",
    "    final_date_lst= list(remlst) + lstappend\n",
    "\n",
    "    sorted_date_list=[]\n",
    "    for date_string in final_date_lst:\n",
    "        sorted_date_list.append(parse(date_string).strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "    sorted_date_list.sort(key=lambda date: datetime.strptime(date, \"%Y-%m-%d\"))\n",
    "    \n",
    "    return(sorted_date_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e92ccd22-58c2-47af-b5b9-4b2a986fe3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Sahil \n",
    "## Description: adding all the patterns to th method and making a list of all the patterns \n",
    "## How to Call:load_RBM(res) \n",
    "## Comments:1) Made two  functions one consist of all patterns and then loading that patterns in laad_RBM\n",
    "##           2) Made list of names proglang orgcomp email \n",
    "##           3) things that are extracted from pattern will be stored in a list. \n",
    "def make_RBM():\n",
    "    print(\"Uploading Rule Based Model\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    nlp.remove_pipe(\"ner\")\n",
    "    ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "    # # add entity ruler\n",
    "    # nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "\n",
    "    ruler.add_patterns([{'label':'PRROGlANG','pattern':p } for p in create_patterns()])\n",
    "    ruler.add_patterns([{'label':'Org_Comp','pattern':po } for po in create_patterns_org()])\n",
    "    ruler.add_patterns([{'label':'Name','pattern':nam } for nam in create_patterns_name()])\n",
    "    ruler.add_patterns([{'label':'Email','pattern':em } for em in create_patterns_email()])\n",
    "    ruler.add_patterns([{'label':'Mobile_No.','pattern':mn } for mn in create_patterns_mobnum()])\n",
    "    ruler.add_patterns([{'label':'DOB.','pattern':mn } for mn in create_patterns_DOB()])\n",
    "    ruler.to_disk(\"./patterns.jsonl\")\n",
    "    nlp.to_disk(\"./res-model\")\n",
    "    print(\"Rule based Model Done\")\n",
    "\n",
    "def load_RBM(res):\n",
    "    nlp_resdoc = spacy.load(\"res-model\")   \n",
    "    doc1=nlp_resdoc(res)\n",
    "    print(\"\\n\\n \\t\\tPRINTING DATA \\n\\n  ( RULE BASED MODEL)\")\n",
    "    names=[]\n",
    "    programlang=[]\n",
    "    Orgcomp=[]\n",
    "    emails=[]\n",
    "    contactno = []\n",
    "    for ent in doc1.ents:\n",
    "        if (ent.label_ == 'Name'):\n",
    "            names.append(ent.text)\n",
    "        if (ent.label_ == 'PRROGlANG'):\n",
    "            programlang.append(ent.text)\n",
    "        if (ent.label_ == 'Org_Comp'):\n",
    "            Orgcomp.append(ent.text)\n",
    "        if (ent.label_ == 'Email'):\n",
    "            emails.append(ent.text)\n",
    "        if (ent.label_ == 'Mobile_No.'):\n",
    "            contactno.append(ent.text)\n",
    "    print(\"NAMES:\")\n",
    "    for nm in names:\n",
    "        print(\"\\t  \" , nm)\n",
    "        \n",
    "    print(\"SKILLS:\")\n",
    "    for pl in programlang:\n",
    "        print(\"\\t  \" , pl)    \n",
    "        \n",
    "    print(\"Worked at \")\n",
    "    for oc in Orgcomp:\n",
    "        print(\"\\t  \" , oc)   \n",
    "        \n",
    "    print(\"Contact No. \")\n",
    "    for cn in contactno:\n",
    "        print(\"\\t  \" , cn)   \n",
    "    print(\"Email for Correspondence \")\n",
    "    for em in emails:\n",
    "        print(\"\\t  \" , em)   \n",
    "        \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fa719bcd-258f-46ec-adad-5ec7056edcc3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7b1b224-851e-4d58-8cb2-11bb9e19bda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "\n",
    "def phoneno(doc):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"hiiiiii\")\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "\n",
    "    phone_reg = [{\"TEXT\": {\"REGEX\": \"^[\\+\\(]?[0-9][0-9 .\\-\\\\s(\\)]{8,}[0-9]\"}},]\n",
    "    phone_reg1 = [{'ORTH': '('},{'SHAPE': 'ddd'},{'ORTH': ')'},{'SHAPE': 'dddd'},{'ORTH': '-', 'OP': '?'},{'SHAPE': 'dddd'}]\n",
    "    phone_rex3 = [{\"TEXT\": {\"REGEX\": \"^[\\(\\w]+\\)\\S+\\-\\d\"}},]\n",
    "    phone_reg4 = [{'SHAPE': 'ddd'},{'ORTH': '-'},{'SHAPE': 'ddd'},{'ORTH': '-'},{'SHAPE': 'dddd'}]\n",
    "    phone_reg5 = [{'ORTH': '('},{'SHAPE': 'ddd'},{'ORTH': ')'},{'SHAPE': 'ddd'},{'ORTH': '-'},{'SHAPE': 'dddd'}]\n",
    "    phone_rex2 = [{\"TEXT\": {\"REGEX\": \"{^(\\(?\\d{3}\\)?)},{ (\\d{3}-\\d\\d\\d\\d)\"}},]\n",
    "    matcher.add(\"number\", [phone_reg,phone_reg1,phone_rex2,phone_reg4,phone_reg4,phone_reg5])\n",
    "\n",
    " \n",
    "    phonenolist=[]\n",
    "\n",
    "    matches=matcher(doc)\n",
    "    print()\n",
    "    for match_id, start, end in matches:\n",
    "        string_id = nlp.vocab.strings[match_id] \n",
    "        span = doc[start:end]  # The matched span\n",
    "        phonenolist.append(span.text)\n",
    "        print(span.text)\n",
    "    return phonenolist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d068948d-3814-49f9-95e4-dbe949d30409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outputmethod():\n",
    "    filenamepathlist= getallfilenames(path = 'home', getall = 'N')\n",
    "    for i in range(len(filenamepathlist)):\n",
    "        mainlist=[]\n",
    "        filepath = filenamepathlist[i]\n",
    "        converted_text_resume = convertfileintotxt(filepath)\n",
    "        remo_char_punc = removecharacpunc(converted_text_resume)\n",
    "        res_cleantxt = removetabsnemptylines(remo_char_punc)\n",
    "#         print(res_cleantxt)\n",
    "        res_doc = nlp(res_cleantxt)\n",
    "        stopwrd_str = removestopwords(res_doc)\n",
    "        resume = replacelemma(res_doc,stopwrd_str)\n",
    "        companylist = readcompanyname(res_doc)\n",
    "        \n",
    "        Skilllist = readskills(res_doc)\n",
    "        \n",
    "        mobnumber = extractmobnumbers(res_doc)\n",
    "        \n",
    "        email = extractemail(res_doc)\n",
    "       \n",
    "        candidate_name =  extractname(res_cleantxt)\n",
    "        qualifcationlist = extract_qualification(res_doc)\n",
    "        \n",
    "        date_list = extractdates(res_doc)\n",
    "        \n",
    "        \n",
    "        rolelist= rolesdesignation(res_cleantxt)\n",
    "        \n",
    "#         phoneno(res_doc)\n",
    "#         print(\"hello\")\n",
    "        import csv\n",
    "        from itertools import zip_longest\n",
    "        data = [candidate_name,rolelist,companylist, Skilllist,mobnumber,email,qualifcationlist,date_list]\n",
    "        export_data = zip_longest(*data, fillvalue = '')\n",
    "        with open('candidate13.csv', 'w', encoding=\"ISO-8859-1\", newline='') as file:\n",
    "            write = csv.writer(file)\n",
    "            write.writerow((\"Name\",\"Role\",\"Company\", \"Skills\",\"mobnumber\",\"email\",\"qualifcationlist\",\"Date\"))\n",
    "            write.writerows(export_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d01f775-cb7c-4a36-8a91-8761ed655674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peter-Smith.pdf\n",
      "\n",
      "Successfully Read\n",
      "\n",
      "Successfully Read\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'remchrpunc' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-880f64e05b6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moutputmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-4db0dc951dd8>\u001b[0m in \u001b[0;36moutputmethod\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilenamepathlist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mconverted_text_resume\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvertfileintotxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mremo_char_punc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremovecharacpunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted_text_resume\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mres_cleantxt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremovetabsnemptylines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremo_char_punc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#         print(res_cleantxt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-9b9f6f4f3aa8>\u001b[0m in \u001b[0;36mremovecharacpunc\u001b[1;34m(textdata)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mele\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpunc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m              \u001b[0mremchrpunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mele\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mremchrpunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'remchrpunc' referenced before assignment"
     ]
    }
   ],
   "source": [
    "outputmethod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc3af34-ae25-403f-98ce-ae300af14be0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc3db24-75b4-449e-bf2f-ac3d9d475ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ce75ff-6e0d-4d4c-a562-ad0faa803666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a023be8c-f997-4e08-8929-76950f213c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = \"this is a sentense how is the weather Is that going to work \"\n",
    "# target = \"Role\"\n",
    "# words = s.split()\n",
    "# listrole=[]\n",
    "# for i,w in enumerate(words):\n",
    "#     if w == target:\n",
    "#         # next word\n",
    "#         listrole.append(words[i+1])\n",
    "#         listrole.append(words[i+2])\n",
    "# for i in listrole:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbf4605-af4e-4874-8dc5-00f0897f7b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fd1f1b-99f8-4dc9-a85c-68348bb5b3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c852c0-f8f8-4567-b5ef-bf887368d92d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputmethod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf31edd1-241d-42cf-ae45-a373ee6fbf42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776f9af7-9dd9-42c3-8c45-b4e1586ce8bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9c9812-16f9-451c-bcec-2fe65a5cad72",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"this is a sentense how is the weather Is that going to work \"\n",
    "target = \"is\"\n",
    "words = s.split()\n",
    "listrole=[]\n",
    "for i,w in enumerate(words):\n",
    "    if w == target:\n",
    "        # next word\n",
    "         listrole.append(words[i+1])\n",
    "    if i>0:\n",
    "        # previous wordif i>0:\n",
    "        print(words[i-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65256fc5-3201-4cbd-a989-614e2bc1a041",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296619b6-f859-4ef1-aaf7-55a67bfe5e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2335275-a974-441e-935f-359a0e5ba274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80243ee-f7b6-4ee3-be3d-13c875abcdcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa6b59af-d456-4866-890f-dae1ebf207ad",
   "metadata": {},
   "source": [
    "## METHODS MADE BY REGEX WHERE POSSIBLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a8408f-521c-4e63-833f-23339703531a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REGEX METHOD TO FIND EMAIL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c31ad45-fdc9-42ad-b4e2-d987cf987c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "\n",
    "EMAIL_REG = [{\"TEXT\": {\"REGEX\": \"^[\\w+\\.[a-zA-Z0-9]+@[\\w]+\\.[\\w]+\"}},]\n",
    "\n",
    "\n",
    "\n",
    "matcher.add(\"Email\", [EMAIL_REG])\n",
    "doc=nlp(\"as.Talenil@yahoo.com yhyhy as.Talend@gmail.com | 813-204-0866 | IND123@gmail.com service@yahoo.in sahil@offsoar.com\")\n",
    "matches=matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id] \n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d624c122-72f6-45ed-aa8c-d989c968273e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "phone_reg = [{\"TEXT\": {\"REGEX\": \"^[\\+\\(]?[0-9][0-9 .\\-\\\\s(\\)]{8,}[0-9]\"}},]\n",
    "phone_reg1 = [{'ORTH': '('},{'SHAPE': 'ddd'},{'ORTH': ')'},{'SHAPE': 'dddd'},{'ORTH': '-', 'OP': '?'},{'SHAPE': 'dddd'}]\n",
    "phone_reg1 = [{'ORTH': '('},{'SHAPE': 'ddd'},{'ORTH': ')'},{'SHAPE': 'ddd'},{'ORTH': '-', 'OP': '?'},{'SHAPE': 'dddd'}]\n",
    "phone_rex2 = [{\"TEXT\": {\"REGEX\": \"^[\\(]\\d{3}[\\)]\\d{3}-\\d{4}\"}},]\n",
    "phone_rex3 = [{\"TEXT\": {\"REGEX\": \"^[\\(\\w]+\\)\\S+\\-\\d\"}},]\n",
    "phone_reg4 = [{'SHAPE': 'ddd'},{'ORTH': '-'},{'SHAPE': 'ddd'},{'ORTH': '-'},{'SHAPE': 'dddd'}]\n",
    "phone_reg5 = [{'ORTH': '('},{'SHAPE': 'ddd'},{'ORTH': ')'},{'SHAPE': 'ddd'},{'ORTH': '-'},{'SHAPE': 'dddd'}]\n",
    "matcher.add(\"number\", [phone_reg,phone_reg1,phone_rex2,phone_reg4,phone_reg4])\n",
    "\n",
    "\n",
    "doc2 = nlp(\"\"\"9967011417\n",
    "(415)312-1234\n",
    "+919967011417\n",
    "(914) 368-2698\n",
    "(967)981-9827 _______\n",
    "+91(976)006-4000\n",
    "020 7183 8750\n",
    "813-204-0866\n",
    "+14155552671\n",
    "(123) 4567 8901\n",
    "+447890123456\n",
    "(408) 960-5241 \n",
    "â€ª\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "matches=matcher(res_doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id] \n",
    "    span = doc2[start:end]  # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafb95ca-758d-4c3e-9995-0ae0cf33854b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{'ORTH': '('},\n",
    "           {'SHAPE': 'ddd'},\n",
    "           {'ORTH': ')'},\n",
    "           {'SHAPE': 'dddd'},\n",
    "           {'ORTH': '-', 'OP': '?'},\n",
    "           {'SHAPE': 'dddd'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0cc7ca-fc4b-4df5-a0df-6725c45a9978",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"phonenumber\", [pattern])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf56d9f6-f66a-4ade-9d12-ab7b71879d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc = nlp(\"call me at (123) 5496 7820\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a699cc48-d7e2-4a7c-ba70-57e0d23c7122",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matcher(doc)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18295b38-ade7-4767-9d65-9885a5791a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id] \n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d6b19c-1ee6-4854-a166-57132f881faf",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a55386-ef44-402f-a9ae-25500d5e6887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9d3d3b-7b55-43e3-b1d2-0319b2833456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10338b2f-5dde-4207-93fb-fd55a199ed89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
